{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP1b1Gb65myCQsfEBHWIafq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SpectraGbes/Spectragbes/blob/main/WQPD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install requests beautifulsoup4 pandas pyarrow tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_hzIFLcvoS3",
        "outputId": "74718632-dad5-4ba1-d132-e2b86545fd69"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.12/dist-packages (18.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2026.1.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.15.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import time\n",
        "import csv\n",
        "import sys\n",
        "import math\n",
        "import shutil\n",
        "import zipfile\n",
        "import logging\n",
        "import traceback\n",
        "from io import StringIO\n",
        "from urllib.parse import urljoin, urlparse\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ----------------------------\n",
        "# CONFIG\n",
        "# ----------------------------\n",
        "DATASET_PAGE = \"https://ihp-wins.unesco.org/dataset/unep-gems-water-global-freshwater-quality-archive\"\n",
        "OUTPUT_DIR = \"gems_data\"            # where CSVs will be stored\n",
        "COMBINED_OUT_CSV = \"gems_combined.csv\"\n",
        "COMBINED_OUT_PARQUET = \"gems_combined.parquet\"\n",
        "\n",
        "# OPTIONAL: to download only some parameters. Leave empty [] to fetch ALL CSV links on the page.\n",
        "# Match is case-insensitive, applied to link text or file name.\n",
        "PARAMETERS_FILTER = [\n",
        "    # Examples (uncomment what you need, or add your own):\n",
        "    # \"pH\",\n",
        "    # \"Turbidity\",\n",
        "    # \"Electrical Conductance\",  # sometimes called \"Specific conductance\" elsewhere; here it's \"Electrical Conductance\"\n",
        "    # \"Dissolved Oxygen\",\n",
        "    # \"Chloride\",\n",
        "    # \"Nitrate\",\n",
        "]\n",
        "\n",
        "# Maximum number of CSVs to download (0 for unlimited)\n",
        "MAX_FILES = 0\n",
        "\n",
        "# Network settings\n",
        "TIMEOUT_S = 180\n",
        "RETRIES = 3\n",
        "BACKOFF_S = 3\n",
        "\n",
        "# Logging\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\")\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# HELPERS\n",
        "# ----------------------------\n",
        "def fetch_html(url, timeout=TIMEOUT_S):\n",
        "    \"\"\"Fetch a page and return BeautifulSoup object.\"\"\"\n",
        "    logging.info(f\"Fetching dataset page: {url}\")\n",
        "    r = requests.get(url, timeout=timeout)\n",
        "    r.raise_for_status()\n",
        "    return BeautifulSoup(r.text, \"html.parser\")\n",
        "\n",
        "\n",
        "def find_csv_links(soup, base_url):\n",
        "    \"\"\"\n",
        "    Parse the dataset page for CSV links.\n",
        "    Return list of (text, absolute_url).\n",
        "    \"\"\"\n",
        "    links = []\n",
        "    for a in soup.find_all(\"a\", href=True):\n",
        "        href = a[\"href\"]\n",
        "        # Some links may include params; check file extension conservatively\n",
        "        if href.lower().endswith(\".csv\"):\n",
        "            url_abs = href if href.startswith(\"http\") else urljoin(base_url, href)\n",
        "            text = a.get_text(strip=True) or os.path.basename(urlparse(url_abs).path)\n",
        "            links.append((text, url_abs))\n",
        "    # Deduplicate by URL\n",
        "    seen = set()\n",
        "    unique = []\n",
        "    for t, u in links:\n",
        "        if u not in seen:\n",
        "            unique.append((t, u))\n",
        "            seen.add(u)\n",
        "    logging.info(f\"Found {len(unique)} CSV links on the page.\")\n",
        "    return unique\n",
        "\n",
        "\n",
        "def matches_filter(text_or_filename, filters):\n",
        "    \"\"\"\n",
        "    Case-insensitive substring match against any filter token.\n",
        "    If filters empty, always True (no filtering).\n",
        "    \"\"\"\n",
        "    if not filters:\n",
        "        return True\n",
        "    s = text_or_filename.lower()\n",
        "    for f in filters:\n",
        "        if f.lower() in s:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def download_with_retries(url, out_path, retries=RETRIES, backoff=BACKOFF_S, timeout=TIMEOUT_S):\n",
        "    \"\"\"Download a file with retries and a progress bar.\"\"\"\n",
        "    for attempt in range(1, retries + 1):\n",
        "        try:\n",
        "            with requests.get(url, stream=True, timeout=timeout) as r:\n",
        "                r.raise_for_status()\n",
        "                total = int(r.headers.get(\"Content-Length\", 0))\n",
        "                desc = os.path.basename(out_path) or \"download\"\n",
        "                with open(out_path, \"wb\") as f, tqdm(\n",
        "                    total=total if total > 0 else None,\n",
        "                    unit=\"B\",\n",
        "                    unit_scale=True,\n",
        "                    desc=f\"Downloading {desc}\",\n",
        "                    leave=False,\n",
        "                ) as pbar:\n",
        "                    for chunk in r.iter_content(chunk_size=1024 * 1024):\n",
        "                        if chunk:\n",
        "                            f.write(chunk)\n",
        "                            if total > 0:\n",
        "                                pbar.update(len(chunk))\n",
        "            return out_path\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"Attempt {attempt}/{retries} failed for {url}: {e}\")\n",
        "            if attempt == retries:\n",
        "                raise\n",
        "            time.sleep(backoff * attempt)\n",
        "\n",
        "\n",
        "def normalize_columns(df):\n",
        "    \"\"\"\n",
        "    Basic normalization across GEMS parameter CSVs.\n",
        "    The exact column names in the archive are consistent per resource, but can differ across parameters.\n",
        "    We try to standardize common fields for ML:\n",
        "      - timestamp (DateTime or SampleDate)\n",
        "      - station/site id\n",
        "      - parameter name\n",
        "      - value\n",
        "      - unit\n",
        "      - lat/lon if present\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    # Candidate timestamp columns\n",
        "    ts_candidates = [\n",
        "        \"DateTime\", \"Date_Time\", \"Date\", \"Datetime\", \"SampleDate\", \"SamplingDate\",\n",
        "        \"ActivityStartDate\", \"ActivityDate\", \"date_time\", \"sample_datetime\", \"time\"\n",
        "    ]\n",
        "    for c in ts_candidates:\n",
        "        if c in df.columns:\n",
        "            df[\"timestamp\"] = pd.to_datetime(df[c], errors=\"coerce\", utc=True)\n",
        "            break\n",
        "    if \"timestamp\" not in df.columns:\n",
        "        # No recognizable timestamp; try to compose if separate date/time present\n",
        "        date_cols = [c for c in df.columns if \"date\" in c.lower()]\n",
        "        time_cols = [c for c in df.columns if \"time\" in c.lower()]\n",
        "        if date_cols and time_cols:\n",
        "            df[\"timestamp\"] = pd.to_datetime(\n",
        "                df[date_cols[0]].astype(str) + \" \" + df[time_cols[0]].astype(str),\n",
        "                errors=\"coerce\", utc=True\n",
        "            )\n",
        "        else:\n",
        "            df[\"timestamp\"] = pd.NaT\n",
        "\n",
        "    # Station/Location candidates\n",
        "    station_candidates = [\n",
        "        \"StationID\", \"StationCode\", \"MonitoringLocationIdentifier\",\n",
        "        \"SiteID\", \"SiteCode\", \"Station\", \"LocationID\", \"StationIdentifier\", \"id\"\n",
        "    ]\n",
        "    for c in station_candidates:\n",
        "        if c in df.columns:\n",
        "            df[\"station_id\"] = df[c].astype(str)\n",
        "            break\n",
        "    if \"station_id\" not in df.columns:\n",
        "        df[\"station_id\"] = None\n",
        "\n",
        "    # Parameter/Characteristic candidates\n",
        "    param_candidates = [\n",
        "        \"ParameterName\", \"CharacteristicName\", \"Parameter\", \"Analyte\", \"Variable\"\n",
        "    ]\n",
        "    for c in param_candidates:\n",
        "        if c in df.columns:\n",
        "            df[\"parameter\"] = df[c].astype(str)\n",
        "            break\n",
        "    if \"parameter\" not in df.columns:\n",
        "        # Try to infer from file name later\n",
        "        df[\"parameter\"] = None\n",
        "\n",
        "    # Value candidates\n",
        "    value_candidates = [\n",
        "        \"ResultValue\", \"Value\", \"Result.MeasureValue\", \"MeasureValue\",\n",
        "        \"ResultMeasureValue\", \"Result\", \"ObservationValue\"\n",
        "    ]\n",
        "    for c in value_candidates:\n",
        "        if c in df.columns:\n",
        "            df[\"value\"] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "            break\n",
        "    if \"value\" not in df.columns:\n",
        "        df[\"value\"] = pd.NA\n",
        "\n",
        "    # Unit candidates\n",
        "    unit_candidates = [\n",
        "        \"Unit\", \"Units\", \"MeasureUnit\", \"ResultUnit\", \"ResultMeasureUnitCode\",\n",
        "        \"UnitName\"\n",
        "    ]\n",
        "    for c in unit_candidates:\n",
        "        if c in df.columns:\n",
        "            df[\"unit\"] = df[c].astype(str)\n",
        "            break\n",
        "    if \"unit\" not in df.columns:\n",
        "        df[\"unit\"] = None\n",
        "\n",
        "    # Latitude/Longitude\n",
        "    lat_candidates = [\"Latitude\", \"LatitudeMeasure\", \"Lat\", \"station_latitude\"]\n",
        "    lon_candidates = [\"Longitude\", \"LongitudeMeasure\", \"Long\", \"station_longitude\"]\n",
        "    for c in lat_candidates:\n",
        "        if c in df.columns:\n",
        "            df[\"latitude\"] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "            break\n",
        "    if \"latitude\" not in df.columns:\n",
        "        df[\"latitude\"] = pd.NA\n",
        "    for c in lon_candidates:\n",
        "        if c in df.columns:\n",
        "            df[\"longitude\"] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "            break\n",
        "    if \"longitude\" not in df.columns:\n",
        "        df[\"longitude\"] = pd.NA\n",
        "\n",
        "    # Sort by timestamp if present\n",
        "    if df[\"timestamp\"].notna().any():\n",
        "        df = df.sort_values(\"timestamp\")\n",
        "\n",
        "    # Keep a slim schema first, but preserve original columns at the end for traceability\n",
        "    cols = [\"timestamp\", \"station_id\", \"parameter\", \"value\", \"unit\", \"latitude\", \"longitude\"]\n",
        "    # Append all other columns for audit/tracking\n",
        "    tail = [c for c in df.columns if c not in cols]\n",
        "    return df[cols + tail]\n",
        "\n",
        "\n",
        "def infer_parameter_from_filename(filename):\n",
        "    name = os.path.splitext(os.path.basename(filename))[0]\n",
        "    # Clean underscores/dashes and make it human-readable\n",
        "    name = re.sub(r\"[ _\\-]+\", \" \", name).strip()\n",
        "    return name\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# MAIN PIPELINE\n",
        "# ----------------------------\n",
        "def main():\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "    # Option A: auto-scrape CSVs from the dataset page\n",
        "    soup = fetch_html(DATASET_PAGE)\n",
        "    all_csvs = find_csv_links(soup, DATASET_PAGE)\n",
        "\n",
        "    # Filter by chosen parameters if provided\n",
        "    filtered = []\n",
        "    for text, url in all_csvs:\n",
        "        # Check link text and filename\n",
        "        fname = os.path.basename(urlparse(url).path)\n",
        "        if matches_filter(text, PARAMETERS_FILTER) or matches_filter(fname, PARAMETERS_FILTER):\n",
        "            filtered.append((text, url))\n",
        "    # If no filter chosen, download everything found\n",
        "    if PARAMETERS_FILTER and not filtered:\n",
        "        logging.warning(\"No links matched your PARAMETERS_FILTER. Falling back to ALL CSVs.\")\n",
        "        filtered = all_csvs\n",
        "    if not PARAMETERS_FILTER:\n",
        "        filtered = all_csvs\n",
        "\n",
        "    # Respect MAX_FILES if set\n",
        "    if MAX_FILES and len(filtered) > MAX_FILES:\n",
        "        filtered = filtered[:MAX_FILES]\n",
        "\n",
        "    logging.info(f\"{len(filtered)} CSV(s) selected for download.\")\n",
        "\n",
        "    downloaded_files = []\n",
        "    for (text, url) in filtered:\n",
        "        filename = os.path.basename(urlparse(url).path)\n",
        "        if not filename.lower().endswith(\".csv\") or filename == \"\":\n",
        "            # Final fallback to a sensible name\n",
        "            filename = re.sub(r\"\\\\W+\", \"_\", text or \"data\") + \".csv\"\n",
        "        out_path = os.path.join(OUTPUT_DIR, filename)\n",
        "        if os.path.exists(out_path) and os.path.getsize(out_path) > 0:\n",
        "            logging.info(f\"Already downloaded: {filename}\")\n",
        "        else:\n",
        "            logging.info(f\"Downloading: {text} -> {filename}\")\n",
        "            try:\n",
        "                download_with_retries(url, out_path)\n",
        "            except Exception:\n",
        "                logging.error(f\"Failed to download {url}\\n{traceback.format_exc()}\")\n",
        "                continue\n",
        "        downloaded_files.append(out_path)\n",
        "\n",
        "    if not downloaded_files:\n",
        "        logging.error(\"No CSVs were downloaded. Check the dataset page or your filters.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # Load & normalize, also infer parameter from file if missing\n",
        "    frames = []\n",
        "    for fp in downloaded_files:\n",
        "        try:\n",
        "            df = pd.read_csv(fp)\n",
        "            df = normalize_columns(df)\n",
        "            # If parameter column is empty, infer from filename once\n",
        "            if df[\"parameter\"].isna().all() or (df[\"parameter\"].astype(str) == \"None\").all():\n",
        "                df[\"parameter\"] = infer_parameter_from_filename(fp)\n",
        "            frames.append(df)\n",
        "            logging.info(f\"Loaded {os.path.basename(fp)} — rows: {len(df)}\")\n",
        "        except Exception:\n",
        "            logging.error(f\"Error reading {fp}\\n{traceback.format_exc()}\")\n",
        "\n",
        "    if not frames:\n",
        "        logging.error(\"No data frames were created from the downloaded CSVs.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    combined = pd.concat(frames, ignore_index=True)\n",
        "\n",
        "    # Optional: basic quality filters for ML\n",
        "    # Drop rows with no value or timestamp\n",
        "    before = len(combined)\n",
        "    # combined = combined.dropna(subset=[\"value\"])\n",
        "    # Keep timestamp if present; you can relax this if needed\n",
        "    # if \"timestamp\" in combined.columns:\n",
        "    #     combined = combined.dropna(subset=[\"timestamp\"])\n",
        "\n",
        "    logging.info(f\"Combined rows before cleanup: {before} | after cleanup: {len(combined)}\")\n",
        "\n",
        "    # Save outputs\n",
        "    combined.to_csv(COMBINED_OUT_CSV, index=False)\n",
        "    logging.info(f\"Saved combined CSV -> {COMBINED_OUT_CSV}\")\n",
        "\n",
        "    # Parquet (smaller & faster for ML)\n",
        "    try:\n",
        "        combined.to_parquet(COMBINED_OUT_PARQUET, index=False)\n",
        "        logging.info(f\"Saved combined Parquet -> {COMBINED_OUT_PARQUET}\")\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"Parquet save failed (install pyarrow): {e}\")\n",
        "\n",
        "    # Quick preview\n",
        "    print(\"\\n=== Preview (first 10 rows) ===\")\n",
        "    print(combined.head(10))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dvh6v841CilB",
        "outputId": "5e016c8c-822a-473c-981d-14aa2495586f"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1153237744.py:300: DtypeWarning: Columns (6,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(fp)\n",
            "/tmp/ipython-input-1153237744.py:300: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(fp)\n",
            "/tmp/ipython-input-1153237744.py:300: DtypeWarning: Columns (6,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(fp)\n",
            "/tmp/ipython-input-1153237744.py:300: DtypeWarning: Columns (6,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(fp)\n",
            "/tmp/ipython-input-1153237744.py:300: DtypeWarning: Columns (6,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(fp)\n",
            "/tmp/ipython-input-1153237744.py:300: DtypeWarning: Columns (6,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(fp)\n",
            "/tmp/ipython-input-1153237744.py:300: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(fp)\n",
            "/tmp/ipython-input-1153237744.py:300: DtypeWarning: Columns (6,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(fp)\n",
            "/tmp/ipython-input-1153237744.py:300: DtypeWarning: Columns (6,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(fp)\n",
            "/tmp/ipython-input-1153237744.py:300: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(fp)\n",
            "/tmp/ipython-input-1153237744.py:300: DtypeWarning: Columns (6,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(fp)\n",
            "/tmp/ipython-input-1153237744.py:300: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(fp)\n",
            "/tmp/ipython-input-1153237744.py:300: DtypeWarning: Columns (6,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(fp)\n",
            "/tmp/ipython-input-1153237744.py:300: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(fp)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Preview (first 10 rows) ===\n",
            "                  timestamp station_id             parameter     value  unit  \\\n",
            "0 1963-08-01 00:00:00+00:00   IND02067  processed alkalinity   96.2541  mg/l   \n",
            "1 1963-08-01 00:00:00+00:00   IND02067  processed alkalinity    4.9800  mg/l   \n",
            "2 1963-09-01 00:00:00+00:00   IND02067  processed alkalinity    2.4900  mg/l   \n",
            "3 1963-09-01 00:00:00+00:00   IND02067  processed alkalinity   90.0000  mg/l   \n",
            "4 1963-10-01 00:00:00+00:00   IND02067  processed alkalinity  162.9100  mg/l   \n",
            "5 1963-10-01 00:00:00+00:00   IND02067  processed alkalinity   24.9000  mg/l   \n",
            "6 1963-11-01 00:00:00+00:00   IND02067  processed alkalinity  122.4430  mg/l   \n",
            "7 1963-11-01 00:00:00+00:00   IND02067  processed alkalinity    2.4900  mg/l   \n",
            "8 1963-11-09 00:00:00+00:00   IND02225  processed alkalinity    0.0000  mg/l   \n",
            "9 1963-11-09 00:00:00+00:00   IND02225  processed alkalinity  144.2620  mg/l   \n",
            "\n",
            "    latitude  longitude   Latitude  Longitude              time        id  \\\n",
            "0  25.330000  83.040000  25.330000  83.040000  08/01/1963 00:00  IND02067   \n",
            "1  25.330000  83.040000  25.330000  83.040000  08/01/1963 00:00  IND02067   \n",
            "2  25.330000  83.040000  25.330000  83.040000  09/01/1963 00:00  IND02067   \n",
            "3  25.330000  83.040000  25.330000  83.040000  09/01/1963 00:00  IND02067   \n",
            "4  25.330000  83.040000  25.330000  83.040000  10/01/1963 00:00  IND02067   \n",
            "5  25.330000  83.040000  25.330000  83.040000  10/01/1963 00:00  IND02067   \n",
            "6  25.330000  83.040000  25.330000  83.040000  11/01/1963 00:00  IND02067   \n",
            "7  25.330000  83.040000  25.330000  83.040000  11/01/1963 00:00  IND02067   \n",
            "8  24.533056  83.049722  24.533056  83.049722  11/09/1963 00:00  IND02225   \n",
            "9  24.533056  83.049722  24.533056  83.049722  11/09/1963 00:00  IND02225   \n",
            "\n",
            "   Depth Parameter Code Analysis Method Code Value Flags     Value  Unit  \\\n",
            "0    0.3        Alk-Tot                  NaN         NaN   96.2541  mg/l   \n",
            "1    0.3       Alk-Phen                  NaN         NaN    4.9800  mg/l   \n",
            "2    0.3       Alk-Phen                  NaN         NaN    2.4900  mg/l   \n",
            "3    0.3        Alk-Tot                  NaN         NaN   90.0000  mg/l   \n",
            "4    0.3        Alk-Tot                  NaN         NaN  162.9100  mg/l   \n",
            "5    0.3       Alk-Phen                  NaN         NaN   24.9000  mg/l   \n",
            "6    0.3        Alk-Tot                  NaN         NaN  122.4430  mg/l   \n",
            "7    0.3       Alk-Phen                  NaN         NaN    2.4900  mg/l   \n",
            "8    0.3       Alk-Phen                  NaN         NaN    0.0000  mg/l   \n",
            "9    0.3        Alk-Tot                  NaN         NaN  144.2620  mg/l   \n",
            "\n",
            "  Data Quality  \n",
            "0      Unknown  \n",
            "1      Unknown  \n",
            "2      Unknown  \n",
            "3      Unknown  \n",
            "4      Unknown  \n",
            "5      Unknown  \n",
            "6      Unknown  \n",
            "7      Unknown  \n",
            "8      Unknown  \n",
            "9      Unknown  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"gems_combined.csv\")\n",
        "print(df.info())\n",
        "print(df['parameter'].value_counts().head(20))\n",
        "print(df['station_id'].nunique(), \"stations\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AecVinZKCnyw",
        "outputId": "3848c941-3fa6-433b-ee08-4980131577db"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3252709269.py:3: DtypeWarning: Columns (13,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(\"gems_combined.csv\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 7560953 entries, 0 to 7560952\n",
            "Data columns (total 18 columns):\n",
            " #   Column                Dtype  \n",
            "---  ------                -----  \n",
            " 0   timestamp             object \n",
            " 1   station_id            object \n",
            " 2   parameter             object \n",
            " 3   value                 float64\n",
            " 4   unit                  object \n",
            " 5   latitude              float64\n",
            " 6   longitude             float64\n",
            " 7   Latitude              float64\n",
            " 8   Longitude             float64\n",
            " 9   time                  object \n",
            " 10  id                    object \n",
            " 11  Depth                 float64\n",
            " 12  Parameter Code        object \n",
            " 13  Analysis Method Code  object \n",
            " 14  Value Flags           object \n",
            " 15  Value                 float64\n",
            " 16  Unit                  object \n",
            " 17  Data Quality          object \n",
            "dtypes: float64(7), object(11)\n",
            "memory usage: 1.0+ GB\n",
            "None\n",
            "parameter\n",
            "processed dissolved gas             713528\n",
            "processed carbon                    566817\n",
            "processed electrical conductance    480784\n",
            "processed indicator organism        424175\n",
            "processed chloride                  372053\n",
            "processed calcium                   309421\n",
            "processed magnesium                 304312\n",
            "processed lead                      300696\n",
            "processed cadmium                   299115\n",
            "processed hardness                  297284\n",
            "processed nickel                    288649\n",
            "processed chromium                  284157\n",
            "processed alkalinity                265617\n",
            "processed arsenic                   256933\n",
            "processed iron                      252731\n",
            "processed halocarbon                247281\n",
            "processed mercury                   229612\n",
            "processed bicarbonate               223146\n",
            "processed copper                    195479\n",
            "processed manganese                 150396\n",
            "Name: count, dtype: int64\n",
            "13417 stations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna(subset=['timestamp', 'value'])\n",
        "df = df[df['value'] >= 0]\n",
        "\n",
        "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "df = df.sort_values('timestamp')\n",
        "\n",
        "# Example: resample to daily averages\n",
        "daily = df.set_index('timestamp').groupby('parameter')['value'].resample('1D').mean().reset_index()"
      ],
      "metadata": {
        "id": "vQaXZEEpDj4R"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['hour'] = df['timestamp'].dt.hour\n",
        "df['day'] = df['timestamp'].dt.day\n",
        "df['month'] = df['timestamp'].dt.month\n",
        "df['dayofweek'] = df['timestamp'].dt.dayofweek"
      ],
      "metadata": {
        "id": "879qB8gXECHK"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['turbidity_lag1'] = df['value'].shift(1)\n",
        "df['turbidity_lag2'] = df['value'].shift(2)\n",
        "df['turbidity_lag24'] = df['value'].shift(24)\n"
      ],
      "metadata": {
        "id": "LhoPnMPzEHNe"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['turbidity_roll_mean_3'] = df['value'].rolling(3).mean()\n",
        "df['turbidity_roll_std_3'] = df['value'].rolling(3).std()\n",
        "df['turbidity_roll_mean_24'] = df['value'].rolling(24).mean()\n"
      ],
      "metadata": {
        "id": "jOViMtBHEMyO"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Drop rows with NaNs from lag/rolling features\n",
        "data = df.dropna()\n",
        "\n",
        "X = data[['hour','day','month','dayofweek',\n",
        "          'turbidity_lag1','turbidity_lag24',\n",
        "          'turbidity_roll_mean_3','turbidity_roll_mean_24']]\n",
        "y = data['value']   # target = future turbidity\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
        "\n",
        "model = RandomForestRegressor(n_estimators=300)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "pred = model.predict(X_test)\n",
        "print(\"MAE:\", mean_absolute_error(y_test, pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SE9KWL7EQmf",
        "outputId": "6d9ecf33-6fae-4085-ae3b-8df1b0aead9e"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE: 6.872360895472459\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "id": "d1f59348",
        "outputId": "2a0d5acb-129d-4420-9240-5a05ec5c17c1"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_loaded = pd.read_csv('gems_combined.csv')\n",
        "print(\"DataFrame Head:\")\n",
        "display(df_loaded.head())\n",
        "print(\"\\nDataFrame Info:\")\n",
        "df_loaded.info()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame Head:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [timestamp, station_id, parameter, value, unit, latitude, longitude, Latitude, Longitude, time, id, Depth, Parameter Code, Analysis Method Code, Value Flags, Value, Unit, Data Quality]\n",
              "Index: []"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-96e5f99d-c633-4265-99dd-2fcb399af61e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>timestamp</th>\n",
              "      <th>station_id</th>\n",
              "      <th>parameter</th>\n",
              "      <th>value</th>\n",
              "      <th>unit</th>\n",
              "      <th>latitude</th>\n",
              "      <th>longitude</th>\n",
              "      <th>Latitude</th>\n",
              "      <th>Longitude</th>\n",
              "      <th>time</th>\n",
              "      <th>id</th>\n",
              "      <th>Depth</th>\n",
              "      <th>Parameter Code</th>\n",
              "      <th>Analysis Method Code</th>\n",
              "      <th>Value Flags</th>\n",
              "      <th>Value</th>\n",
              "      <th>Unit</th>\n",
              "      <th>Data Quality</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-96e5f99d-c633-4265-99dd-2fcb399af61e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-96e5f99d-c633-4265-99dd-2fcb399af61e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-96e5f99d-c633-4265-99dd-2fcb399af61e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "repr_error": "Out of range float values are not JSON compliant: nan"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DataFrame Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 0 entries\n",
            "Data columns (total 18 columns):\n",
            " #   Column                Non-Null Count  Dtype \n",
            "---  ------                --------------  ----- \n",
            " 0   timestamp             0 non-null      object\n",
            " 1   station_id            0 non-null      object\n",
            " 2   parameter             0 non-null      object\n",
            " 3   value                 0 non-null      object\n",
            " 4   unit                  0 non-null      object\n",
            " 5   latitude              0 non-null      object\n",
            " 6   longitude             0 non-null      object\n",
            " 7   Latitude              0 non-null      object\n",
            " 8   Longitude             0 non-null      object\n",
            " 9   time                  0 non-null      object\n",
            " 10  id                    0 non-null      object\n",
            " 11  Depth                 0 non-null      object\n",
            " 12  Parameter Code        0 non-null      object\n",
            " 13  Analysis Method Code  0 non-null      object\n",
            " 14  Value Flags           0 non-null      object\n",
            " 15  Value                 0 non-null      object\n",
            " 16  Unit                  0 non-null      object\n",
            " 17  Data Quality          0 non-null      object\n",
            "dtypes: object(18)\n",
            "memory usage: 132.0+ bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "joblib.dump(model, \"turbidity_model.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8ciyDh7EW8U",
        "outputId": "72d0bbdf-50f1-4b56-fe99-937e0b26236a"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['turbidity_model.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evp-obIrKUJp",
        "outputId": "b770acf2-411e-4f4c-f680-a614219861ca"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.53.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=5.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.2.4)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.3.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<13,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.46)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.5.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (4.26.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2.15.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2026.1.4)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.0.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.30.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.53.1-py3-none-any.whl (9.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m82.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.53.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import joblib\n",
        "import pandas as pd\n",
        "\n",
        "model = joblib.load(\"turbidity_model.pkl\")\n",
        "\n",
        "st.title(\"AI Water Quality Predictor\")\n",
        "\n",
        "uploaded = st.file_uploader(\"Upload latest water data CSV\")\n",
        "\n",
        "if uploaded:\n",
        "    df = pd.read_csv(uploaded)\n",
        "    # preprocess same as training...\n",
        "    pred = model.predict(df)\n",
        "    st.line_chart(pred)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19_DPUQWKFDp",
        "outputId": "76417f77-4f34-423c-d2c4-d0af15e988f6"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2026-01-25 03:59:36.762 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-25 03:59:36.764 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-25 03:59:36.766 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-25 03:59:36.768 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-25 03:59:36.770 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-25 03:59:36.773 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-25 03:59:36.779 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-25 03:59:36.781 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2026-01-25 03:59:36.782 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0877551"
      },
      "source": [
        "Please replace `YOUR_NGROK_AUTH_TOKEN` with your actual ngrok authtoken from [https://dashboard.ngrok.com/get-started/your-authtoken](https://dashboard.ngrok.com/get-started/your-authtoken) and run the cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53de6581"
      },
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Replace 'YOUR_NGROK_AUTH_TOKEN' with your actual authtoken\n",
        "ngrok.set_auth_token(\"38jUXztDRrcg7NDyuJGe8YsZWiW_7nkbtMVLdK1LS6ScbktRr\")"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "new_cell_b2f36560",
        "outputId": "86cffe6a-eeb8-48ef-babf-633e243aaaa8"
      },
      "source": [
        "from pyngrok import ngrok\n",
        "import subprocess\n",
        "import os\n",
        "import time\n",
        "\n",
        "# Terminate any existing ngrok tunnels\n",
        "ngrok.kill()\n",
        "\n",
        "# Start a Streamlit app in the background and capture its output\n",
        "# Redirect stdout/stderr to a file to prevent Colab from hanging\n",
        "log_file = open(\"streamlit_log.txt\", \"w\")\n",
        "streamlit_process = subprocess.Popen([\"streamlit\", \"run\", \"app.py\"],\n",
        "                                     stdout=log_file, stderr=log_file,\n",
        "                                     env=os.environ.copy())\n",
        "\n",
        "print(\"Streamlit app started in background. Waiting for it to become available...\")\n",
        "time.sleep(5) # Give Streamlit some time to start\n",
        "\n",
        "# Get the public URL\n",
        "try:\n",
        "    public_url = ngrok.connect(8501)\n",
        "    print(f\"Streamlit App URL: {public_url}\")\n",
        "except Exception as e:\n",
        "    print(f\"Failed to establish ngrok tunnel: {e}\")\n",
        "    print(\"Check 'streamlit_log.txt' for Streamlit app errors if the above steps fail.\")"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit app started in background. Waiting for it to become available...\n",
            "Streamlit App URL: NgrokTunnel: \"https://acrogenic-nontransitionally-shin.ngrok-free.dev\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tail -n 200 ~/.streamlit/logs/* 2>/dev/null || true"
      ],
      "metadata": {
        "id": "J6OiPzuzVV7e"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tail -n 200 streamlit_log.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9shwog-VWu-",
        "outputId": "5591b68e-e79f-4c08-9b7b-a2bb995953f0"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\n",
            "\n",
            "  You can now view your Streamlit app in your browser.\n",
            "\n",
            "  Local URL: http://localhost:8508\n",
            "  Network URL: http://172.28.0.12:8508\n",
            "  External URL: http://34.186.111.12:8508\n",
            "\n"
          ]
        }
      ]
    }
  ]
}